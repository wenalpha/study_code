{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd0dfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing sst dataset ...\n",
      "loading sst tokenizer...\n",
      "loading embedding_matrix: 300_sst_embedding_matrix.pkl\n",
      "n_trainable_params: 161305, n_nontrainable_params: 4527000\n",
      "> training arguments:\n",
      ">>> model_name: lstm\n",
      ">>> dataset: sst\n",
      ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
      ">>> initializer: <function xavier_uniform_ at 0x00000240CD39B7F0>\n",
      ">>> learning_rate: 0.003\n",
      ">>> l2reg: 1e-05\n",
      ">>> num_epoch: 100\n",
      ">>> batch_size: 32\n",
      ">>> log_step: 50\n",
      ">>> embed_dim: 300\n",
      ">>> hidden_dim: 100\n",
      ">>> polarities_dim: 5\n",
      ">>> save: False\n",
      ">>> seed: 776\n",
      ">>> device: cuda\n",
      ">>> model_class: <class 'models.lstm.LSTM'>\n",
      ">>> inputs_cols: ['text_indices']\n",
      "cuda memory allocated: 19520512\n",
      "repeat:  1\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n",
      "loss: 1.5778, acc: 0.3438, test_acc: 0.2861, test_f1: 0.1431\n",
      "loss: 1.4188, acc: 0.4219, test_acc: 0.2761, test_f1: 0.1122\n",
      "loss: 1.1602, acc: 0.4375, test_acc: 0.3665, test_f1: 0.1976\n",
      "loss: 1.2178, acc: 0.4531, test_acc: 0.3912, test_f1: 0.2620\n",
      "loss: 1.2586, acc: 0.4375, test_acc: 0.4159, test_f1: 0.3623\n",
      "max_val_acc： 0.41590493601462525 \tmax_val_f1： 0.36229923566524097\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 1.1726, acc: 0.5312, test_acc: 0.4013, test_f1: 0.3780\n",
      "loss: 1.1943, acc: 0.4688, test_acc: 0.4086, test_f1: 0.3052\n",
      "loss: 1.1393, acc: 0.4479, test_acc: 0.4415, test_f1: 0.3719\n",
      "loss: 1.5029, acc: 0.4297, test_acc: 0.4351, test_f1: 0.3800\n",
      "loss: 1.1683, acc: 0.4500, test_acc: 0.4013, test_f1: 0.3100\n",
      "max_val_acc： 0.44149908592321757 \tmax_val_f1： 0.3800014982094664\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      "loss: 0.9496, acc: 0.5938, test_acc: 0.4022, test_f1: 0.3317\n",
      "loss: 1.2410, acc: 0.4844, test_acc: 0.4461, test_f1: 0.3736\n",
      "loss: 1.0565, acc: 0.5000, test_acc: 0.4525, test_f1: 0.4040\n",
      "loss: 1.0452, acc: 0.5234, test_acc: 0.4625, test_f1: 0.4396\n",
      "loss: 1.4212, acc: 0.4750, test_acc: 0.4433, test_f1: 0.3784\n",
      "max_val_acc： 0.4625228519195612 \tmax_val_f1： 0.43959779632774953\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 1.3112, acc: 0.5000, test_acc: 0.4516, test_f1: 0.4190\n",
      "loss: 1.0744, acc: 0.5000, test_acc: 0.4333, test_f1: 0.3915\n",
      "loss: 1.1174, acc: 0.5208, test_acc: 0.4479, test_f1: 0.4254\n",
      "loss: 1.2376, acc: 0.5078, test_acc: 0.4598, test_f1: 0.4314\n",
      "loss: 0.9613, acc: 0.5188, test_acc: 0.4525, test_f1: 0.4242\n",
      "loss: 0.9090, acc: 0.5469, test_acc: 0.4570, test_f1: 0.4267\n",
      "max_val_acc： 0.4625228519195612 \tmax_val_f1： 0.43959779632774953\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.5961, acc: 0.7500, test_acc: 0.4324, test_f1: 0.4064\n",
      "loss: 0.8607, acc: 0.6875, test_acc: 0.4516, test_f1: 0.4099\n",
      "loss: 1.2387, acc: 0.5938, test_acc: 0.4406, test_f1: 0.3968\n",
      "loss: 1.0449, acc: 0.5703, test_acc: 0.4406, test_f1: 0.3821\n",
      "loss: 1.0269, acc: 0.5687, test_acc: 0.4543, test_f1: 0.4364\n",
      "max_val_acc： 0.4625228519195612 \tmax_val_f1： 0.43959779632774953\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.7578, acc: 0.8438, test_acc: 0.4378, test_f1: 0.4208\n",
      "loss: 0.5537, acc: 0.8438, test_acc: 0.4442, test_f1: 0.4315\n",
      "loss: 1.0580, acc: 0.7188, test_acc: 0.4342, test_f1: 0.4277\n",
      "loss: 0.6249, acc: 0.7344, test_acc: 0.4580, test_f1: 0.4321\n",
      "loss: 0.8697, acc: 0.7063, test_acc: 0.4388, test_f1: 0.4339\n",
      "max_val_acc： 0.4625228519195612 \tmax_val_f1： 0.43959779632774953\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.4105, acc: 0.8750, test_acc: 0.4351, test_f1: 0.4215\n",
      "loss: 0.5737, acc: 0.8125, test_acc: 0.4159, test_f1: 0.4186\n",
      "loss: 0.6002, acc: 0.8021, test_acc: 0.4305, test_f1: 0.4138\n",
      "loss: 0.9192, acc: 0.7344, test_acc: 0.4250, test_f1: 0.4239\n",
      "loss: 0.3829, acc: 0.7750, test_acc: 0.4342, test_f1: 0.4269\n",
      "loss: 0.4380, acc: 0.7812, test_acc: 0.4360, test_f1: 0.4278\n",
      "max_val_acc： 0.4625228519195612 \tmax_val_f1： 0.43959779632774953\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n",
      "loss: 0.3760, acc: 0.8750, test_acc: 0.4378, test_f1: 0.4345\n",
      "loss: 0.1737, acc: 0.9219, test_acc: 0.4113, test_f1: 0.4018\n",
      "loss: 0.4649, acc: 0.8958, test_acc: 0.4278, test_f1: 0.4094\n",
      "loss: 0.3767, acc: 0.8750, test_acc: 0.4260, test_f1: 0.4208\n",
      "loss: 0.6640, acc: 0.8438, test_acc: 0.4205, test_f1: 0.4143\n",
      "early stop.\n",
      "test_acc: 0.40454545454545454     test_f1: 0.39296252588465763\n",
      "####################################################################################################\n",
      "repeat:  2\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n",
      "loss: 1.4999, acc: 0.5000, test_acc: 0.2669, test_f1: 0.0950\n",
      "loss: 1.4915, acc: 0.4531, test_acc: 0.3437, test_f1: 0.2086\n",
      "loss: 1.9079, acc: 0.3958, test_acc: 0.3739, test_f1: 0.2184\n",
      "loss: 1.4673, acc: 0.3750, test_acc: 0.3693, test_f1: 0.2649\n",
      "loss: 1.1934, acc: 0.3750, test_acc: 0.4059, test_f1: 0.3222\n",
      "max_val_acc： 0.40585009140767825 \tmax_val_f1： 0.32221353709611533\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 1.2274, acc: 0.4688, test_acc: 0.3803, test_f1: 0.2687\n",
      "loss: 1.3979, acc: 0.4219, test_acc: 0.4278, test_f1: 0.3774\n",
      "loss: 1.4633, acc: 0.3854, test_acc: 0.4104, test_f1: 0.2923\n",
      "loss: 1.2060, acc: 0.4219, test_acc: 0.4406, test_f1: 0.4050\n",
      "loss: 1.2984, acc: 0.4250, test_acc: 0.4479, test_f1: 0.4295\n",
      "max_val_acc： 0.44789762340036565 \tmax_val_f1： 0.4295139642950737\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      "loss: 1.4650, acc: 0.3750, test_acc: 0.4186, test_f1: 0.3066\n",
      "loss: 1.0344, acc: 0.5000, test_acc: 0.4333, test_f1: 0.3656\n",
      "loss: 1.2041, acc: 0.5104, test_acc: 0.4369, test_f1: 0.3824\n",
      "loss: 1.1528, acc: 0.5078, test_acc: 0.4671, test_f1: 0.4402\n",
      "loss: 1.4621, acc: 0.5000, test_acc: 0.4506, test_f1: 0.3824\n",
      "max_val_acc： 0.4670932358318099 \tmax_val_f1： 0.4402035894150986\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 1.0881, acc: 0.5938, test_acc: 0.4552, test_f1: 0.4088\n",
      "loss: 0.8859, acc: 0.6406, test_acc: 0.4314, test_f1: 0.3923\n",
      "loss: 1.1377, acc: 0.5938, test_acc: 0.4589, test_f1: 0.4349\n",
      "loss: 1.1588, acc: 0.5469, test_acc: 0.4351, test_f1: 0.4084\n",
      "loss: 1.1312, acc: 0.5500, test_acc: 0.4662, test_f1: 0.4398\n",
      "loss: 0.9354, acc: 0.5365, test_acc: 0.4324, test_f1: 0.4114\n",
      "max_val_acc： 0.4670932358318099 \tmax_val_f1： 0.4402035894150986\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.8482, acc: 0.6562, test_acc: 0.4580, test_f1: 0.4431\n",
      "loss: 0.9344, acc: 0.6562, test_acc: 0.4516, test_f1: 0.4427\n",
      "loss: 1.0495, acc: 0.6354, test_acc: 0.4534, test_f1: 0.4220\n",
      "loss: 0.8775, acc: 0.6172, test_acc: 0.4488, test_f1: 0.4263\n",
      "loss: 1.0507, acc: 0.6125, test_acc: 0.4570, test_f1: 0.4289\n",
      "max_val_acc： 0.4670932358318099 \tmax_val_f1： 0.44312111372042323\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.6415, acc: 0.7500, test_acc: 0.4305, test_f1: 0.4074\n",
      "loss: 0.6506, acc: 0.7500, test_acc: 0.4360, test_f1: 0.4266\n",
      "loss: 0.8627, acc: 0.6979, test_acc: 0.4278, test_f1: 0.3854\n",
      "loss: 0.6074, acc: 0.7422, test_acc: 0.4269, test_f1: 0.4096\n",
      "loss: 0.8029, acc: 0.7438, test_acc: 0.4442, test_f1: 0.4166\n",
      "max_val_acc： 0.4670932358318099 \tmax_val_f1： 0.44312111372042323\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.4800, acc: 0.8438, test_acc: 0.4397, test_f1: 0.4311\n",
      "loss: 0.4052, acc: 0.8438, test_acc: 0.4415, test_f1: 0.4300\n",
      "loss: 0.6988, acc: 0.8021, test_acc: 0.4214, test_f1: 0.3964\n",
      "loss: 0.3925, acc: 0.8125, test_acc: 0.4351, test_f1: 0.4158\n",
      "loss: 0.4479, acc: 0.8313, test_acc: 0.4433, test_f1: 0.4331\n",
      "loss: 0.6265, acc: 0.8125, test_acc: 0.4360, test_f1: 0.4271\n",
      "max_val_acc： 0.4670932358318099 \tmax_val_f1： 0.44312111372042323\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5073, acc: 0.8125, test_acc: 0.4470, test_f1: 0.4264\n",
      "loss: 0.2595, acc: 0.8750, test_acc: 0.4342, test_f1: 0.4286\n",
      "loss: 0.4127, acc: 0.8646, test_acc: 0.4333, test_f1: 0.4163\n",
      "loss: 0.5973, acc: 0.8438, test_acc: 0.4388, test_f1: 0.4318\n",
      "loss: 0.4976, acc: 0.8250, test_acc: 0.4397, test_f1: 0.4308\n",
      "max_val_acc： 0.4670932358318099 \tmax_val_f1： 0.44312111372042323\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  8\n",
      "loss: 0.2137, acc: 0.9688, test_acc: 0.4141, test_f1: 0.3950\n",
      "loss: 0.2057, acc: 0.9375, test_acc: 0.4223, test_f1: 0.4112\n",
      "loss: 0.2820, acc: 0.9167, test_acc: 0.4141, test_f1: 0.4089\n",
      "loss: 0.0980, acc: 0.9375, test_acc: 0.4378, test_f1: 0.4362\n",
      "loss: 0.1289, acc: 0.9437, test_acc: 0.4369, test_f1: 0.4272\n",
      "max_val_acc： 0.4670932358318099 \tmax_val_f1： 0.44312111372042323\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  9\n",
      "loss: 0.2497, acc: 0.9375, test_acc: 0.4232, test_f1: 0.4200\n",
      "loss: 0.0982, acc: 0.9531, test_acc: 0.4205, test_f1: 0.4151\n",
      "loss: 0.1633, acc: 0.9375, test_acc: 0.4314, test_f1: 0.4237\n",
      "loss: 0.1148, acc: 0.9453, test_acc: 0.4342, test_f1: 0.4165\n",
      "loss: 0.2310, acc: 0.9313, test_acc: 0.4196, test_f1: 0.4056\n",
      "loss: 0.0667, acc: 0.9427, test_acc: 0.4406, test_f1: 0.4216\n",
      "early stop.\n",
      "test_acc: 0.41     test_f1: 0.388495625957235\n",
      "####################################################################################################\n",
      "repeat:  3\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n",
      "loss: 1.6000, acc: 0.0312, test_acc: 0.2943, test_f1: 0.2096\n",
      "loss: 1.5011, acc: 0.1719, test_acc: 0.3080, test_f1: 0.1828\n",
      "loss: 1.1240, acc: 0.2708, test_acc: 0.3739, test_f1: 0.2793\n",
      "loss: 1.1912, acc: 0.3516, test_acc: 0.3867, test_f1: 0.2724\n",
      "loss: 1.2101, acc: 0.3875, test_acc: 0.3711, test_f1: 0.2970\n",
      "max_val_acc： 0.386654478976234 \tmax_val_f1： 0.2970407712730744\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 1.1139, acc: 0.4375, test_acc: 0.4196, test_f1: 0.3570\n",
      "loss: 1.2315, acc: 0.4844, test_acc: 0.4205, test_f1: 0.3373\n",
      "loss: 1.2019, acc: 0.4688, test_acc: 0.4232, test_f1: 0.3614\n",
      "loss: 1.1906, acc: 0.4688, test_acc: 0.4397, test_f1: 0.3701\n",
      "loss: 1.1964, acc: 0.4750, test_acc: 0.4205, test_f1: 0.3299\n",
      "max_val_acc： 0.4396709323583181 \tmax_val_f1： 0.3701009921153978\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      "loss: 1.1206, acc: 0.5625, test_acc: 0.4598, test_f1: 0.4035\n",
      "loss: 1.2774, acc: 0.5312, test_acc: 0.4561, test_f1: 0.4180\n",
      "loss: 0.9632, acc: 0.5312, test_acc: 0.4260, test_f1: 0.3748\n",
      "loss: 1.5339, acc: 0.5078, test_acc: 0.4305, test_f1: 0.3917\n",
      "loss: 1.2857, acc: 0.4938, test_acc: 0.4525, test_f1: 0.4033\n",
      "max_val_acc： 0.45978062157221206 \tmax_val_f1： 0.4180300220046137\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 1.1338, acc: 0.5625, test_acc: 0.4452, test_f1: 0.4281\n",
      "loss: 0.6707, acc: 0.6406, test_acc: 0.4707, test_f1: 0.4481\n",
      "loss: 1.1897, acc: 0.5833, test_acc: 0.4506, test_f1: 0.4263\n",
      "loss: 1.0902, acc: 0.5781, test_acc: 0.4653, test_f1: 0.4372\n",
      "loss: 0.9008, acc: 0.5875, test_acc: 0.4369, test_f1: 0.3703\n",
      "loss: 1.0917, acc: 0.5833, test_acc: 0.4543, test_f1: 0.4288\n",
      "max_val_acc： 0.47074954296160876 \tmax_val_f1： 0.4481041097311579\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.7695, acc: 0.6875, test_acc: 0.4479, test_f1: 0.4027\n",
      "loss: 0.7781, acc: 0.6719, test_acc: 0.4415, test_f1: 0.4119\n",
      "loss: 1.0035, acc: 0.6458, test_acc: 0.4634, test_f1: 0.4300\n",
      "loss: 0.9005, acc: 0.6562, test_acc: 0.4707, test_f1: 0.4506\n",
      "loss: 0.8735, acc: 0.6312, test_acc: 0.4662, test_f1: 0.4364\n",
      "max_val_acc： 0.47074954296160876 \tmax_val_f1： 0.45060158021025815\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.7645, acc: 0.6875, test_acc: 0.4580, test_f1: 0.4463\n",
      "loss: 0.5837, acc: 0.7344, test_acc: 0.4543, test_f1: 0.4471\n",
      "loss: 0.9252, acc: 0.7083, test_acc: 0.4470, test_f1: 0.4327\n",
      "loss: 0.6295, acc: 0.7266, test_acc: 0.4424, test_f1: 0.4257\n",
      "loss: 1.0565, acc: 0.6813, test_acc: 0.4470, test_f1: 0.4240\n",
      "max_val_acc： 0.47074954296160876 \tmax_val_f1： 0.45060158021025815\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.5059, acc: 0.8438, test_acc: 0.4534, test_f1: 0.4367\n",
      "loss: 0.5118, acc: 0.8125, test_acc: 0.4324, test_f1: 0.4149\n",
      "loss: 0.4288, acc: 0.8333, test_acc: 0.4296, test_f1: 0.4195\n",
      "loss: 0.6935, acc: 0.7812, test_acc: 0.4351, test_f1: 0.4200\n",
      "loss: 0.4927, acc: 0.7750, test_acc: 0.4516, test_f1: 0.4326\n",
      "loss: 0.4140, acc: 0.7812, test_acc: 0.4570, test_f1: 0.4430\n",
      "max_val_acc： 0.47074954296160876 \tmax_val_f1： 0.45060158021025815\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n",
      "loss: 0.2266, acc: 0.9375, test_acc: 0.4214, test_f1: 0.4045\n",
      "loss: 0.1308, acc: 0.9531, test_acc: 0.4333, test_f1: 0.4299\n",
      "loss: 0.4506, acc: 0.9062, test_acc: 0.4223, test_f1: 0.3920\n",
      "loss: 0.2794, acc: 0.8984, test_acc: 0.4269, test_f1: 0.4204\n",
      "loss: 0.4556, acc: 0.8750, test_acc: 0.4360, test_f1: 0.4219\n",
      "max_val_acc： 0.47074954296160876 \tmax_val_f1： 0.45060158021025815\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  8\n",
      "loss: 0.4364, acc: 0.7812, test_acc: 0.4342, test_f1: 0.4050\n",
      "loss: 0.1021, acc: 0.8906, test_acc: 0.4388, test_f1: 0.4181\n",
      "loss: 0.1704, acc: 0.8958, test_acc: 0.4314, test_f1: 0.4163\n",
      "loss: 0.1433, acc: 0.9219, test_acc: 0.4342, test_f1: 0.4178\n",
      "loss: 0.1460, acc: 0.9250, test_acc: 0.4333, test_f1: 0.4108\n",
      "max_val_acc： 0.47074954296160876 \tmax_val_f1： 0.45060158021025815\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  9\n",
      "loss: 0.1649, acc: 0.9688, test_acc: 0.4159, test_f1: 0.4089\n",
      "loss: 0.0438, acc: 0.9844, test_acc: 0.4214, test_f1: 0.3959\n",
      "loss: 0.0850, acc: 0.9896, test_acc: 0.4132, test_f1: 0.3828\n",
      "loss: 0.1776, acc: 0.9844, test_acc: 0.4196, test_f1: 0.3822\n",
      "loss: 0.1420, acc: 0.9812, test_acc: 0.4104, test_f1: 0.4050\n",
      "loss: 0.1960, acc: 0.9635, test_acc: 0.4223, test_f1: 0.4013\n",
      "early stop.\n",
      "test_acc: 0.4222727272727273     test_f1: 0.4078801498519466\n",
      "####################################################################################################\n",
      "max_test_acc_avg: 0.4122727272727273\n",
      "max_test_f1_avg: 0.3964461005646131\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bucket_iterator import BucketIterator\n",
    "from sklearn import metrics\n",
    "from data_utils import DatesetReader\n",
    "from models import TextCNN, LSTM\n",
    "\n",
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "        dataset = DatesetReader(dataset=opt.dataset, embed_dim=opt.embed_dim)\n",
    "\n",
    "        self.train_data_loader = BucketIterator(data=dataset.train_data, batch_size=opt.batch_size, shuffle=True)\n",
    "        self.valid_data_loader = BucketIterator(data=dataset.valid_data, batch_size=opt.batch_size, shuffle=True)\n",
    "        self.test_data_loader = BucketIterator(data=dataset.test_data, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "        self.model = opt.model_class(dataset.embedding_matrix, opt).to(opt.device)\n",
    "        self._print_args()\n",
    "        self.global_f1 = 0.\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print('cuda memory allocated:', torch.cuda.memory_allocated(device=opt.device.index))\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape)).item()\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        print('n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
    "        print('> training arguments:')\n",
    "        for arg in vars(self.opt):\n",
    "            print('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for p in self.model.parameters():\n",
    "            if p.requires_grad:\n",
    "                if len(p.shape) > 1:\n",
    "                    self.opt.initializer(p)\n",
    "                else:\n",
    "                    stdv = 1. / math.sqrt(p.shape[0])\n",
    "                    torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def _train(self, criterion, optimizer):\n",
    "        max_val_acc = 0\n",
    "        max_val_f1 = 0\n",
    "        global_step = 0\n",
    "        continue_not_increase = 0\n",
    "        for epoch in range(self.opt.num_epoch):\n",
    "            print('>' * 100)\n",
    "            print('epoch: ', epoch)\n",
    "            n_correct, n_total = 0, 0\n",
    "            increase_flag = False\n",
    "            for i_batch, sample_batched in enumerate(self.train_data_loader):\n",
    "                global_step += 1\n",
    "\n",
    "                # switch model to training mode, clear gradient accumulators\n",
    "                self.model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs = [sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                targets = sample_batched['polarity'].to(self.opt.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                    n_total += len(outputs)\n",
    "                    train_acc = n_correct / n_total\n",
    "\n",
    "                    #中间过程使用验证集进行验证，得出最佳模型并保存\n",
    "                    val_acc, val_f1 = self._evaluate_acc_f1(self.valid_data_loader)\n",
    "                    if val_acc > max_val_acc:\n",
    "                        max_val_acc = val_acc\n",
    "                    if val_f1 > max_val_f1:\n",
    "                        increase_flag = True\n",
    "                        max_val_f1 = val_f1\n",
    "                        if self.opt.save and val_f1 > self.global_f1:\n",
    "                            self.global_f1 = val_f1\n",
    "                            torch.save(self.model.state_dict(), 'state_dict/'+self.opt.model_name+'_'+self.opt.dataset+'.pkl')\n",
    "                            print('>>> best model saved.')\n",
    "                    print('loss: {:.4f}, acc: {:.4f}, test_acc: {:.4f}, test_f1: {:.4f}'.format(loss.item(), train_acc, val_acc, val_f1))\n",
    "            if increase_flag == False:\n",
    "                continue_not_increase += 1\n",
    "                if continue_not_increase >= 5:\n",
    "                    print('early stop.')\n",
    "                    break\n",
    "            else:\n",
    "                continue_not_increase = 0\n",
    "            print('max_val_acc：', max_val_acc, '\\tmax_val_f1：', max_val_f1)\n",
    "        #使用测试集合测试模型效果\n",
    "        test_acc, test_f1 = self._evaluate_acc_f1(self.test_data_loader)\n",
    "        return test_acc, test_f1\n",
    "\n",
    "    def _evaluate_acc_f1(self, data_loader):\n",
    "        # switch model to evaluation mode\n",
    "        self.model.eval()\n",
    "        n_test_correct, n_test_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        with torch.no_grad():\n",
    "            for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "                t_inputs = [t_sample_batched[col].to(opt.device) for col in self.opt.inputs_cols]\n",
    "                t_targets = t_sample_batched['polarity'].to(opt.device)\n",
    "                t_outputs = self.model(t_inputs)\n",
    "\n",
    "                n_test_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_test_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "        test_acc = n_test_correct / n_test_total\n",
    "        labels = [0, 1, 2, 3, 4]#根据不同分类来修改\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=labels, average='macro')\n",
    "        return test_acc, f1\n",
    "\n",
    "    def run(self, repeats=3):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        if not os.path.exists('log/'):\n",
    "            os.mkdir('log/')\n",
    "\n",
    "        f_out = open('log/'+self.opt.model_name+'_'+self.opt.dataset+'_val.txt', 'w', encoding='utf-8')\n",
    "\n",
    "        test_acc_avg = 0\n",
    "        test_f1_avg = 0#recall , precisionf1 = 2 * pr * r / (p +ｒ)\n",
    "        for i in range(repeats):\n",
    "            print('repeat: ', (i+1))\n",
    "            f_out.write('repeat: '+str(i+1))\n",
    "            self._reset_params()\n",
    "            _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "            optimizer = self.opt.optimizer(_params, lr=self.opt.learning_rate, weight_decay=self.opt.l2reg)\n",
    "            test_acc, test_f1 = self._train(criterion, optimizer)\n",
    "            print('test_acc: {0}     test_f1: {1}'.format(test_acc, test_f1))\n",
    "            f_out.write('test_acc: {0}, test_f1: {1}'.format(test_acc, test_f1))\n",
    "            test_acc_avg += test_acc\n",
    "            test_f1_avg += test_f1\n",
    "            print('#' * 100)\n",
    "        print(\"max_test_acc_avg:\", test_acc_avg / repeats)\n",
    "        print(\"max_test_f1_avg:\", test_f1_avg / repeats)\n",
    "\n",
    "        f_out.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyper Parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', default='lstm', type=str, help='textcnn, lstm')\n",
    "    parser.add_argument('--dataset', default='sst', type=str, help='sst, cola')\n",
    "    parser.add_argument('--optimizer', default='adam', type=str)\n",
    "    parser.add_argument('--initializer', default='xavier_uniform_', type=str)\n",
    "    parser.add_argument('--learning_rate', default=0.003, type=float)\n",
    "    parser.add_argument('--l2reg', default=0.00001, type=float)\n",
    "    parser.add_argument('--num_epoch', default=100, type=int)\n",
    "    parser.add_argument('--batch_size', default=32, type=int)\n",
    "    parser.add_argument('--log_step', default=50, type=int)\n",
    "    parser.add_argument('--embed_dim', default=300, type=int)\n",
    "    parser.add_argument('--hidden_dim', default=100, type=int)\n",
    "    parser.add_argument('--polarities_dim', default=5, type=int)\n",
    "    parser.add_argument('--save', default=False, type=bool)\n",
    "    parser.add_argument('--seed', default=776, type=int)\n",
    "    parser.add_argument('--device', default=None, type=str)\n",
    "    opt = parser.parse_args([])\n",
    "\n",
    "    model_classes = {\n",
    "        'textcnn': TextCNN,\n",
    "        'lstm': LSTM,\n",
    "    }\n",
    "    input_colses = {\n",
    "        'textcnn': ['text_indices'],\n",
    "        'lstm': ['text_indices'],\n",
    "    }\n",
    "    initializers = {\n",
    "        'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "        'xavier_normal_': torch.nn.init.xavier_normal_,\n",
    "        'orthogonal_': torch.nn.init.orthogonal_,\n",
    "    }\n",
    "    optimizers = {\n",
    "        'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
    "        'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "        'adam': torch.optim.Adam,  # default lr=0.001\n",
    "        'adamax': torch.optim.Adamax,  # default lr=0.002\n",
    "        'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "        'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
    "        'sgd': torch.optim.SGD,\n",
    "    }\n",
    "    opt.model_class = model_classes[opt.model_name]\n",
    "    opt.inputs_cols = input_colses[opt.model_name]\n",
    "    opt.initializer = initializers[opt.initializer]\n",
    "    opt.optimizer = optimizers[opt.optimizer]\n",
    "    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
    "        if opt.device is None else torch.device(opt.device)\n",
    "\n",
    "    if opt.seed is not None:\n",
    "        random.seed(opt.seed)\n",
    "        numpy.random.seed(opt.seed)\n",
    "        torch.manual_seed(opt.seed)\n",
    "        torch.cuda.manual_seed(opt.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    ins = Instructor(opt)\n",
    "    ins.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad3d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
